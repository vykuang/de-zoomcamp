{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FHV retrieval prototype\n",
    "\n",
    "source: https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-01.parquet, for all 2019\n",
    "\n",
    "Can partition the url to\n",
    "\n",
    "- base: https://d37ci6vzurychx.cloudfront.net/trip-data/\n",
    "- filename: `<taxi_type>_tripdata_<yyyy>-<mm>.parqet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from logging import getLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = getLogger(name=\"fhv.ipynb\")\n",
    "def fetch(dataset_url: str) -> pd.DataFrame:\n",
    "    \"\"\"Read taxi data in parquet format from web and \n",
    "    return as dataframe\n",
    "\n",
    "    Set retries=3 to get around web traffic jitters\n",
    "    \"\"\"\n",
    "    # logger = get_run_logger()\n",
    "    df = pd.read_parquet(dataset_url, engine='pyarrow')\n",
    "    logger.info(f\"{len(df)} rows loaded from url\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "urls = defaultdict(str)\n",
    "urls[0] = \"https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-01.parquet\"\n",
    "# df = fetch(urls[0])\n",
    "# print(f'num records in fhv jan: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropOff_datetime</th>\n",
       "      <th>PUlocationID</th>\n",
       "      <th>DOlocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00001</td>\n",
       "      <td>2019-01-01 00:30:00</td>\n",
       "      <td>2019-01-01 02:51:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00001</td>\n",
       "      <td>2019-01-01 00:45:00</td>\n",
       "      <td>2019-01-01 00:54:49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00001</td>\n",
       "      <td>2019-01-01 00:15:00</td>\n",
       "      <td>2019-01-01 00:54:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00008</td>\n",
       "      <td>2019-01-01 00:19:00</td>\n",
       "      <td>2019-01-01 00:39:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00008</td>\n",
       "      <td>2019-01-01 00:27:00</td>\n",
       "      <td>2019-01-01 00:37:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dispatching_base_num     pickup_datetime    dropOff_datetime  PUlocationID  \\\n",
       "0               B00001 2019-01-01 00:30:00 2019-01-01 02:51:55           NaN   \n",
       "1               B00001 2019-01-01 00:45:00 2019-01-01 00:54:49           NaN   \n",
       "2               B00001 2019-01-01 00:15:00 2019-01-01 00:54:52           NaN   \n",
       "3               B00008 2019-01-01 00:19:00 2019-01-01 00:39:00           NaN   \n",
       "4               B00008 2019-01-01 00:27:00 2019-01-01 00:37:00           NaN   \n",
       "\n",
       "   DOlocationID  SR_Flag Affiliated_base_number  \n",
       "0           NaN      NaN                 B00001  \n",
       "1           NaN      NaN                 B00001  \n",
       "2           NaN      NaN                 B00001  \n",
       "3           NaN      NaN                 B00008  \n",
       "4           NaN      NaN                 B00008  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dispatching_base_num              object\n",
       "pickup_datetime           datetime64[ns]\n",
       "dropOff_datetime          datetime64[ns]\n",
       "PUlocationID                     float64\n",
       "DOlocationID                     float64\n",
       "SR_Flag                          float64\n",
       "Affiliated_base_number            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datatypes are all a-okay. Try with `pd.io.ql.get_schema`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE \"fhv_taxi_data\" (\n",
      "\"dispatching_base_num\" TEXT,\n",
      "  \"pickup_datetime\" TIMESTAMP,\n",
      "  \"dropOff_datetime\" TIMESTAMP,\n",
      "  \"PUlocationID\" REAL,\n",
      "  \"DOlocationID\" REAL,\n",
      "  \"SR_Flag\" REAL,\n",
      "  \"Affiliated_base_number\" TEXT\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pd.io.sql.get_schema(df, name='fhv_taxi_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Expected a local filesystem path, got a URI: 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-02.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m urls[\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-02.parquet\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m table \u001b[39m=\u001b[39m pq\u001b[39m.\u001b[39;49mread_table(urls[\u001b[39m1\u001b[39;49m])\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/de-zoomcamp-ofDTZRjf-py3.10/lib/python3.10/site-packages/pyarrow/parquet/core.py:2824\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, memory_map, read_dictionary, filesystem, filters, buffer_size, partitioning, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[1;32m   2817\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2818\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m keyword is no longer supported with the new \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2819\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdatasets-based implementation. Specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2820\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39muse_legacy_dataset=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to temporarily recover the old \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2821\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbehaviour.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2822\u001b[0m     )\n\u001b[1;32m   2823\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2824\u001b[0m     dataset \u001b[39m=\u001b[39m _ParquetDatasetV2(\n\u001b[1;32m   2825\u001b[0m         source,\n\u001b[1;32m   2826\u001b[0m         schema\u001b[39m=\u001b[39;49mschema,\n\u001b[1;32m   2827\u001b[0m         filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[1;32m   2828\u001b[0m         partitioning\u001b[39m=\u001b[39;49mpartitioning,\n\u001b[1;32m   2829\u001b[0m         memory_map\u001b[39m=\u001b[39;49mmemory_map,\n\u001b[1;32m   2830\u001b[0m         read_dictionary\u001b[39m=\u001b[39;49mread_dictionary,\n\u001b[1;32m   2831\u001b[0m         buffer_size\u001b[39m=\u001b[39;49mbuffer_size,\n\u001b[1;32m   2832\u001b[0m         filters\u001b[39m=\u001b[39;49mfilters,\n\u001b[1;32m   2833\u001b[0m         ignore_prefixes\u001b[39m=\u001b[39;49mignore_prefixes,\n\u001b[1;32m   2834\u001b[0m         pre_buffer\u001b[39m=\u001b[39;49mpre_buffer,\n\u001b[1;32m   2835\u001b[0m         coerce_int96_timestamp_unit\u001b[39m=\u001b[39;49mcoerce_int96_timestamp_unit,\n\u001b[1;32m   2836\u001b[0m         thrift_string_size_limit\u001b[39m=\u001b[39;49mthrift_string_size_limit,\n\u001b[1;32m   2837\u001b[0m         thrift_container_size_limit\u001b[39m=\u001b[39;49mthrift_container_size_limit,\n\u001b[1;32m   2838\u001b[0m     )\n\u001b[1;32m   2839\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m   2840\u001b[0m     \u001b[39m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[1;32m   2841\u001b[0m     \u001b[39m# module is not available\u001b[39;00m\n\u001b[1;32m   2842\u001b[0m     \u001b[39mif\u001b[39;00m filters \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/de-zoomcamp-ofDTZRjf-py3.10/lib/python3.10/site-packages/pyarrow/parquet/core.py:2401\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.__init__\u001b[0;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, schema, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, **kwargs)\u001b[0m\n\u001b[1;32m   2399\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m   2400\u001b[0m             filesystem \u001b[39m=\u001b[39m LocalFileSystem(use_mmap\u001b[39m=\u001b[39mmemory_map)\n\u001b[0;32m-> 2401\u001b[0m     \u001b[39mif\u001b[39;00m filesystem\u001b[39m.\u001b[39;49mget_file_info(path_or_paths)\u001b[39m.\u001b[39mis_file:\n\u001b[1;32m   2402\u001b[0m         single_file \u001b[39m=\u001b[39m path_or_paths\n\u001b[1;32m   2403\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/de-zoomcamp-ofDTZRjf-py3.10/lib/python3.10/site-packages/pyarrow/_fs.pyx:571\u001b[0m, in \u001b[0;36mpyarrow._fs.FileSystem.get_file_info\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/de-zoomcamp-ofDTZRjf-py3.10/lib/python3.10/site-packages/pyarrow/error.pxi:144\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/de-zoomcamp-ofDTZRjf-py3.10/lib/python3.10/site-packages/pyarrow/error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Expected a local filesystem path, got a URI: 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-02.parquet'"
     ]
    }
   ],
   "source": [
    "urls[1] = \"https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-02.parquet\"\n",
    "table = pq.read_table(urls[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fhv_tripdata_2019-02.parquet'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = Path(urls[1])\n",
    "fname = fp.name\n",
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m month \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      5\u001b[0m dataset_file \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtaxi_type\u001b[39m}\u001b[39;00m\u001b[39m_tripdata_\u001b[39m\u001b[39m{\u001b[39;00myear\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mmonth\u001b[39m:\u001b[39;00m\u001b[39m02\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m fpath \u001b[39m=\u001b[39m Path(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdata_dir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mtaxi_type\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mdataset_file\u001b[39m}\u001b[39;00m\u001b[39m.parquet\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m local_path \u001b[39m=\u001b[39m write_local(df, fpath)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data/taxi_ingest_data\"\n",
    "taxi_type = \"fhv\"\n",
    "year = 2019\n",
    "month = 1\n",
    "dataset_file = f\"{taxi_type}_tripdata_{year}-{month:02}\"\n",
    "fpath = Path(f\"{data_dir}/{taxi_type}/{dataset_file}.parquet\")\n",
    "local_path = write_local(df, fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import flow, task\n",
    "from prefect_gcp.cloud_storage import GcsBucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task()\n",
    "def upload_gcs(block_name: str, fpath: Path) -> None:\n",
    "    \"\"\"Upload the local parquet file to GCS\"\"\"\n",
    "    gcs_block = GcsBucket.load(block_name)\n",
    "    # this will return <color>/<filename>.parquet\n",
    "    gcs_path = Path(fpath.parts[-2]) / fpath.parts[-1]\n",
    "    gcs_block.upload_from_path(from_path=fpath, to_path=gcs_path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flow()\n",
    "def web_gcs_parq(\n",
    "    taxi_type: str, year: int, month: int, block_name: str, data_dir: str = \"../data/cache\"\n",
    ") -> None:\n",
    "    \"\"\"Main ETL function\"\"\"\n",
    "    dataset_file = f\"{taxi_type}_tripdata_{year}-{month:02}\"\n",
    "    dataset_url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{dataset_file}.parquet\"\n",
    "\n",
    "    fpath = Path(f\"{data_dir}/{taxi_type}/{dataset_file}.parquet\")\n",
    "    if not fpath.exists():\n",
    "        df = fetch(dataset_url)\n",
    "        # df_clean = clean(df)\n",
    "        fpath = write_local(df, fpath)\n",
    "    upload_gcs(block_name, fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:47:21.313 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | prefect.engine - Created flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'secret-turtle'</span> for flow<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> 'web-gcs-parq'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:47:21.313 | \u001b[36mINFO\u001b[0m    | prefect.engine - Created flow run\u001b[35m 'secret-turtle'\u001b[0m for flow\u001b[1;35m 'web-gcs-parq'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:47:22.640 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'secret-turtle'</span> - Created task run 'upload_gcs-bf4ea732-0' for task 'upload_gcs'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:47:22.640 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'secret-turtle'\u001b[0m - Created task run 'upload_gcs-bf4ea732-0' for task 'upload_gcs'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:47:22.644 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'secret-turtle'</span> - Executing 'upload_gcs-bf4ea732-0' immediately...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:47:22.644 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'secret-turtle'\u001b[0m - Executing 'upload_gcs-bf4ea732-0' immediately...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:47:23.285 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'upload_gcs-bf4ea732-0' - Getting bucket 'dtc_data_lake_de-zoom-83'.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:47:23.285 | \u001b[36mINFO\u001b[0m    | Task run 'upload_gcs-bf4ea732-0' - Getting bucket 'dtc_data_lake_de-zoom-83'.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:47:23.361 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'upload_gcs-bf4ea732-0' - Uploading from PosixPath('../data/taxi_ingest_data/fhv/fhv_tripdata_2019-01.parquet') to the bucket 'dtc_data_lake_de-zoom-83' path 'data/fhv/fhv_tripdata_2019-01.parquet'.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:47:23.361 | \u001b[36mINFO\u001b[0m    | Task run 'upload_gcs-bf4ea732-0' - Uploading from PosixPath('../data/taxi_ingest_data/fhv/fhv_tripdata_2019-01.parquet') to the bucket 'dtc_data_lake_de-zoom-83' path 'data/fhv/fhv_tripdata_2019-01.parquet'.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:47:25.014 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'upload_gcs-bf4ea732-0' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:47:25.014 | \u001b[36mINFO\u001b[0m    | Task run 'upload_gcs-bf4ea732-0' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:47:25.208 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'secret-turtle'</span> - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>('All states completed.')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:47:25.208 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'secret-turtle'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m('All states completed.')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Completed(message=None, type=COMPLETED, result=LiteralResult(type='literal', value=None))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_name = \"ny-taxi-gcs\"\n",
    "web_gcs_parq(taxi_type, year, month, block_name, data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 12\n"
     ]
    }
   ],
   "source": [
    "mths = \"1-12\"\n",
    "a, b = list(map(int, mths.split(\"-\")))\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mths = \"12\"\n",
    "if '-' in mths:\n",
    "    a, b = list(map(int, mths.split(\"-\")))\n",
    "else:\n",
    "    a = int(mths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(a, a+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n"
     ]
    }
   ],
   "source": [
    "a = b = 2\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = ['<Blob: dtc_data_lake_de-zoom-83, data/fhv/fhv_tripdata_2019-01.parquet, 1675877819813642>', '<Blob: dtc_data_lake_de-zoom-83, data/fhv/fhv_tripdata_2019-02.parquet, 1675878686204679>']\n",
    "fname = \"fhv_tripdata_2019-02.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/fhv/fhv_tripdata_2019-01.parquet,',\n",
       " 'data/fhv/fhv_tripdata_2019-02.parquet,']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [[fn for fn in blob.split() if '/' in fn][0] for blob in blobs]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "present = [fname in n for n in b]\n",
    "present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-zoomcamp-ofDTZRjf-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5353673437182399eff36f4c16ed56ea0c7acdfe0ac221ac5d31504a99f322a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
