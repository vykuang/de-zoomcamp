{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - homework\n",
    "\n",
    "## 1. Spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pathlib import Path\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/28 14:19:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HVFHW june 2021\n",
    "\n",
    "Output partition size?\n",
    "\n",
    "Download raw dataset and repartition using `cast_partition.py`:\n",
    "\n",
    "```bash\n",
    "wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz\n",
    "python cast_partition.py \\\n",
    "    --taxi_type fhvhv \\\n",
    "    --year 2021 \\\n",
    "    --month 6 \\\n",
    "    --num_partitions 12\n",
    "```\n",
    "\n",
    "24 MB per partition x 12 partitions\n",
    "\n",
    "## 3. Count\n",
    "\n",
    "Records on june 15th?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "taxi_type = \"fhvhv\"\n",
    "year = 2021\n",
    "month = 6\n",
    "parts_dir = Path(\"../data/taxi_ingest_data/parts/\")\n",
    "fpath = parts_dir / taxi_type / f\"{year}\" / f\"{month:02d}\"\n",
    "df = spark.read.parquet(str(fpath))\n",
    "df.createOrReplaceTempView(\"fhvhv_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: float (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  452470|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        count(1)\n",
    "    FROM\n",
    "        fhvhv_data\n",
    "    WHERE\n",
    "        CAST(pickup_datetime AS DATE) = \"2021-06-15\"\n",
    "        \"\"\"\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Longest trip in hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+\n",
      "|             hours|    pickup_datetime|\n",
      "+------------------+-------------------+\n",
      "| 66.86666666666666|2021-06-25 13:55:41|\n",
      "|25.533333333333335|2021-06-22 12:09:45|\n",
      "|19.966666666666665|2021-06-27 10:32:29|\n",
      "|18.183333333333334|2021-06-26 22:37:11|\n",
      "|16.466666666666665|2021-06-23 20:40:43|\n",
      "|14.266666666666667|2021-06-23 22:03:31|\n",
      "|              13.9|2021-06-24 23:11:00|\n",
      "|11.666666666666666|2021-06-04 20:56:02|\n",
      "|             11.35|2021-06-27 07:45:19|\n",
      "|10.983333333333333|2021-06-20 17:05:12|\n",
      "+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    WITH duration as(\n",
    "        SELECT\n",
    "            (dropoff_datetime - pickup_datetime) AS trip_duration,\n",
    "            pickup_datetime\n",
    "        FROM\n",
    "            fhvhv_data\n",
    "        ) \n",
    "    SELECT\n",
    "        EXTRACT(hour from trip_duration) + EXTRACT(day FROM trip_duration) * 24 + EXTRACT(minute FROM trip_duration)/60 AS hours ,\n",
    "        pickup_datetime\n",
    "    FROM\n",
    "        duration\n",
    "    ORDER BY hours DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|((extract(day FROM (TIMESTAMP '2020-01-20 00:00:00' - TIMESTAMP '2020-01-02 00:00:00')) * 24) + extract(hour FROM (TIMESTAMP '2020-01-20 00:00:00' - TIMESTAMP '2020-01-02 00:00:00')))|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                                                                                                    432|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT EXTRACT(day FROM (timestamp '2020-1-20' - timestamp '2020-1-2'))*24+EXTRACT(hour FROM (timestamp '2020-1-20' - timestamp '2020-1-2'))\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Web UI Port\n",
    "\n",
    "default: 4040\n",
    "\n",
    "## 6. Most frequent pickup zone\n",
    "\n",
    "Join with `taxi_zone_lookup.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_zone = parts_dir / \"../taxi_zone_lookup.csv\"\n",
    "df_zone = spark.read \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('inferSchema', 'true') \\\n",
    "    .csv(str(file_zone))\n",
    "df_zone.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone.createOrReplaceTempView('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+--------+-------------------+------------+\n",
      "|PULocationID| count| Borough|               Zone|service_zone|\n",
      "+------------+------+--------+-------------------+------------+\n",
      "|          61|231279|Brooklyn|Crown Heights North|   Boro Zone|\n",
      "+------------+------+--------+-------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .select('PULocationID') \\\n",
    "    .groupBy('PULocationID') \\\n",
    "    .count() \\\n",
    "    .orderBy('count', ascending=False) \\\n",
    "    .limit(1) \\\n",
    "    .join(\n",
    "        df_zone, df.PULocationID == df_zone.LocationID, 'inner'\n",
    "    ) \\\n",
    "    .drop('LocationID') \\\n",
    "    .show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution in SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+----------+---------+-------------------+------------+\n",
      "|PULocationID| count|LocationID|  Borough|               Zone|service_zone|\n",
      "+------------+------+----------+---------+-------------------+------------+\n",
      "|          37|187929|        37| Brooklyn|     Bushwick South|   Boro Zone|\n",
      "|          61|231279|        61| Brooklyn|Crown Heights North|   Boro Zone|\n",
      "|          76|186780|        76| Brooklyn|      East New York|   Boro Zone|\n",
      "|          79|221244|        79|Manhattan|       East Village| Yellow Zone|\n",
      "|         132|188867|       132|   Queens|        JFK Airport|    Airports|\n",
      "+------------+------+----------+---------+-------------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    with hot as (\n",
    "        SELECT\n",
    "            PULocationID,\n",
    "            count(1) as count\n",
    "        FROM\n",
    "            fhvhv_data\n",
    "        GROUP BY\n",
    "            PULocationID\n",
    "        ORDER BY\n",
    "            count DESC\n",
    "        LIMIT 5    \n",
    "    )\n",
    "    SELECT *\n",
    "    FROM hot\n",
    "    INNER JOIN\n",
    "    zones\n",
    "    ON hot.PULocationID = zones.LocationID\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-zoomcamp-ofDTZRjf-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5353673437182399eff36f4c16ed56ea0c7acdfe0ac221ac5d31504a99f322a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
