{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Spark and PySpark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading parquets\n",
    "\n",
    "Instantiate a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/17 22:42:34 WARN Utils: Your hostname, Kohada resolves to a loopback address: 127.0.1.1; using 172.30.125.167 instead (on interface eth0)\n",
      "23/02/17 22:42:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/kohada/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/02/17 22:42:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006794 ../data/taxi_ingest_data/fhvhv_tripdata_2021-01.parquet\n"
     ]
    }
   ],
   "source": [
    "!wc -l ../data/taxi_ingest_data/fhvhv_tripdata_2021-01.parquet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark session reads the parquet in as `pyspark.sql.Dataframe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|hvfhs_license_num|dispatching_base_num|originating_base_num|   request_datetime|  on_scene_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls| bcf|sales_tax|congestion_surcharge|airport_fee|tips|driver_pay|shared_request_flag|shared_match_flag|access_a_ride_flag|wav_request_flag|wav_match_flag|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|           HV0003|              B02682|              B02682|2021-01-01 08:28:09|2021-01-01 08:31:42|2021-01-01 08:33:44|2021-01-01 08:49:07|         230|         166|      5.26|      923|              22.28|  0.0|0.67|     1.98|                2.75|       null| 0.0|     14.99|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02682|              B02682|2021-01-01 08:45:56|2021-01-01 08:55:19|2021-01-01 08:55:19|2021-01-01 09:18:21|         152|         167|      3.65|     1382|              18.36|  0.0|0.55|     1.63|                 0.0|       null| 0.0|     17.06|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2021-01-01 08:21:15|2021-01-01 08:22:41|2021-01-01 08:23:56|2021-01-01 08:38:05|         233|         142|      3.51|      849|              14.05|  0.0|0.48|     1.25|                2.75|       null|0.94|     12.98|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2021-01-01 08:39:12|2021-01-01 08:42:37|2021-01-01 08:42:51|2021-01-01 08:45:50|         142|         143|      0.74|      179|               7.91|  0.0|0.24|      0.7|                2.75|       null| 0.0|      7.41|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2021-01-01 08:46:11|2021-01-01 08:47:17|2021-01-01 08:48:14|2021-01-01 09:08:42|         143|          78|       9.2|     1228|              27.11|  0.0|0.81|     2.41|                2.75|       null| 0.0|     22.44|                  N|                N|                  |               N|             N|\n",
      "|           HV0005|              B02510|                null|2021-01-01 08:04:00|               null|2021-01-01 08:06:59|2021-01-01 08:43:01|          88|          42|     9.725|     2162|              28.11|  0.0|0.84|     2.49|                2.75|       null| 0.0|      28.9|                  N|                N|                 N|               N|             N|\n",
      "|           HV0005|              B02510|                null|2021-01-01 08:40:06|               null|2021-01-01 08:50:00|2021-01-01 09:04:57|          42|         151|     2.469|      897|              25.03|  0.0|0.75|     2.22|                 0.0|       null| 0.0|     15.01|                  N|                N|                 N|               N|             N|\n",
      "|           HV0003|              B02764|              B02764|2021-01-01 08:10:36|2021-01-01 08:12:28|2021-01-01 08:14:30|2021-01-01 08:50:27|          71|         226|     13.53|     2157|              29.67|  0.0|1.04|     3.08|                 0.0|       null| 0.0|      34.2|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02875|              B02875|2021-01-01 08:21:17|2021-01-01 08:22:25|2021-01-01 08:22:54|2021-01-01 08:30:20|         112|         255|       1.6|      446|               6.89|  0.0|0.21|     0.61|                 0.0|       null| 0.0|      6.26|                  N|                N|                  |               N|             N|\n",
      "|           HV0003|              B02875|              B02875|2021-01-01 08:36:57|2021-01-01 08:38:09|2021-01-01 08:40:12|2021-01-01 08:53:31|         255|         232|       3.2|      800|              11.51|  0.0|0.53|     1.03|                2.75|       null|2.82|     10.99|                  N|                N|                  |               N|             N|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fpath = Path(\"../data/taxi_ingest_data/fhvhv_tripdata_2021-01.parquet\")\n",
    "df = spark.read \\\n",
    "    .option('header', 'true') \\\n",
    "    .parquet(str(fpath))\n",
    "df.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparks UI can be accessed via http://localhost:4040/jobs/\n",
    "\n",
    "![spark-ui](./img/spark-ui-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(hvfhs_license_num,StringType,true),StructField(dispatching_base_num,StringType,true),StructField(originating_base_num,StringType,true),StructField(request_datetime,TimestampType,true),StructField(on_scene_datetime,TimestampType,true),StructField(pickup_datetime,TimestampType,true),StructField(dropoff_datetime,TimestampType,true),StructField(PULocationID,LongType,true),StructField(DOLocationID,LongType,true),StructField(trip_miles,DoubleType,true),StructField(trip_time,LongType,true),StructField(base_passenger_fare,DoubleType,true),StructField(tolls,DoubleType,true),StructField(bcf,DoubleType,true),StructField(sales_tax,DoubleType,true),StructField(congestion_surcharge,DoubleType,true),StructField(airport_fee,DoubleType,true),StructField(tips,DoubleType,true),StructField(driver_pay,DoubleType,true),StructField(shared_request_flag,StringType,true),StructField(shared_match_flag,StringType,true),StructField(access_a_ride_flag,StringType,true),StructField(wav_request_flag,StringType,true),StructField(wav_match_flag,StringType,true)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatted:\n",
    "\n",
    "```scala\n",
    "StructType(List(\n",
    "    StructField(hvfhs_license_num,StringType,true),\n",
    "    StructField(dispatching_base_num,StringType,true),\n",
    "    StructField(originating_base_num,StringType,true),\n",
    "    StructField(request_datetime,TimestampType,true),\n",
    "    StructField(on_scene_datetime,TimestampType,true),\n",
    "    StructField(pickup_datetime,TimestampType,true),\n",
    "    StructField(dropoff_datetime,TimestampType,true),\n",
    "    StructField(PULocationID,LongType,true),\n",
    "    StructField(DOLocationID,LongType,true),\n",
    "    StructField(trip_miles,DoubleType,true),\n",
    "    StructField(trip_time,LongType,true),\n",
    "    StructField(base_passenger_fare,DoubleType,true),\n",
    "    StructField(tolls,DoubleType,true),\n",
    "    StructField(bcf,DoubleType,true),\n",
    "    StructField(sales_tax,DoubleType,true),\n",
    "    StructField(congestion_surcharge,DoubleType,true),\n",
    "    StructField(airport_fee,DoubleType,true),\n",
    "    StructField(tips,DoubleType,true),\n",
    "    StructField(driver_pay,DoubleType,true),\n",
    "    StructField(shared_request_flag,StringType,true),\n",
    "    StructField(shared_match_flag,StringType,true),\n",
    "    StructField(access_a_ride_flag,StringType,true),\n",
    "    StructField(wav_request_flag,StringType,true),\n",
    "    StructField(wav_match_flag,StringType,true)\n",
    "    )\n",
    ")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `scala` syntax\n",
    "- Double: float (8 bytes)\n",
    "- Long: int64 (8 bytes)\n",
    "- `true` refers to `is_nullable`\n",
    "\n",
    "So contrary to the `.csv`, the parquet types seem to be correct.\n",
    "\n",
    "Otherwise, Spark can read directly from a `pd.DataFrame`, where we can enforce the schema before feeding to spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1001 ../data/taxi_ingest_data/fhvhv_tripdata_2021-01.parquet > ../data/taxi_ingest_data/fhvhv_tripdata_sample.parquet\n",
    "!tail -n 1 ../data/taxi_ingest_data/fhvhv_tripdata_2021-01.parquet >> ../data/taxi_ingest_data/fhvhv_tripdata_sample.parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difficult to read only parts of a parquet with only pandas\n",
      " Invalid column metadata (corrupt file?)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_pd = pd.read_parquet(fpath.parent / 'fhv_tripdata_sample.parquet',)\n",
    "    df_pd.dtypes\n",
    "except OSError as e:\n",
    "    print(\"Difficult to read only parts of a parquet with only pandas\\n\", e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `pyarrow` directly to access the `nrows` arg\n",
    "\n",
    "[SOF source](https://stackoverflow.com/a/69888274/5496416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num               object\n",
       "dispatching_base_num            object\n",
       "originating_base_num            object\n",
       "request_datetime        datetime64[ns]\n",
       "on_scene_datetime       datetime64[ns]\n",
       "pickup_datetime         datetime64[ns]\n",
       "dropoff_datetime        datetime64[ns]\n",
       "PULocationID                     int64\n",
       "DOLocationID                     int64\n",
       "trip_miles                     float64\n",
       "trip_time                        int64\n",
       "base_passenger_fare            float64\n",
       "tolls                          float64\n",
       "bcf                            float64\n",
       "sales_tax                      float64\n",
       "congestion_surcharge           float64\n",
       "airport_fee                    float64\n",
       "tips                           float64\n",
       "driver_pay                     float64\n",
       "shared_request_flag             object\n",
       "shared_match_flag               object\n",
       "access_a_ride_flag              object\n",
       "wav_request_flag                object\n",
       "wav_match_flag                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyarrow.parquet import ParquetFile\n",
    "import pyarrow as pa\n",
    "\n",
    "# reader interface\n",
    "pf = ParquetFile(fpath)\n",
    "first_nrows = next(pf.iter_batches(batch_size=1000))\n",
    "df_pd = pa.Table.from_batches([first_nrows]).to_pandas()\n",
    "df_pd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', originating_base_num='B02682', request_datetime=datetime.datetime(2021, 1, 1, 0, 28, 9), on_scene_datetime=datetime.datetime(2021, 1, 1, 0, 31, 42), pickup_datetime=datetime.datetime(2021, 1, 1, 0, 33, 44), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 49, 7), PULocationID=230, DOLocationID=166, trip_miles=5.26, trip_time=923, base_passenger_fare=22.28, tolls=0.0, bcf=0.67, sales_tax=1.98, congestion_surcharge=2.75, airport_fee=nan, tips=0.0, driver_pay=14.99, shared_request_flag='N', shared_match_flag='N', access_a_ride_flag=' ', wav_request_flag='N', wav_match_flag='N'),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', originating_base_num='B02682', request_datetime=datetime.datetime(2021, 1, 1, 0, 45, 56), on_scene_datetime=datetime.datetime(2021, 1, 1, 0, 55, 19), pickup_datetime=datetime.datetime(2021, 1, 1, 0, 55, 19), dropoff_datetime=datetime.datetime(2021, 1, 1, 1, 18, 21), PULocationID=152, DOLocationID=167, trip_miles=3.65, trip_time=1382, base_passenger_fare=18.36, tolls=0.0, bcf=0.55, sales_tax=1.63, congestion_surcharge=0.0, airport_fee=nan, tips=0.0, driver_pay=17.06, shared_request_flag='N', shared_match_flag='N', access_a_ride_flag=' ', wav_request_flag='N', wav_match_flag='N'),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', originating_base_num='B02764', request_datetime=datetime.datetime(2021, 1, 1, 0, 21, 15), on_scene_datetime=datetime.datetime(2021, 1, 1, 0, 22, 41), pickup_datetime=datetime.datetime(2021, 1, 1, 0, 23, 56), dropoff_datetime=datetime.datetime(2021, 1, 1, 0, 38, 5), PULocationID=233, DOLocationID=142, trip_miles=3.51, trip_time=849, base_passenger_fare=14.05, tolls=0.0, bcf=0.48, sales_tax=1.25, congestion_surcharge=2.75, airport_fee=nan, tips=0.94, driver_pay=12.98, shared_request_flag='N', shared_match_flag='N', access_a_ride_flag=' ', wav_request_flag='N', wav_match_flag='N')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_pd = spark.createDataFrame(df_pd)\n",
    "df_from_pd.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the `scala` types above and convert it to python via `pyspark.sql.types`, and revise the schema during read. E.g. change `locationID` to `Integer` instead of `Long` to reduce storage requirements, since `Long` takes up 8 bytes (int64) and `Integer` takes up 4 (int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('originating_base_num', types.StringType(), True),\n",
    "    types.StructField('request_datetime', types.TimestampType(), True),\n",
    "    types.StructField('on_scene_datetime', types.TimestampType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.LongType(), True),\n",
    "    types.StructField('DOLocationID', types.LongType(), True),\n",
    "    types.StructField('trip_miles', types.DoubleType(), True),\n",
    "    types.StructField('trip_time', types.LongType(), True),\n",
    "    types.StructField('base_passenger_fare', types.DoubleType(), True),\n",
    "    types.StructField('tolls', types.DoubleType(), True),\n",
    "    types.StructField('bcf', types.DoubleType(), True),\n",
    "    types.StructField('sales_tax', types.DoubleType(), True),\n",
    "    types.StructField('congestion_surcharge', types.DoubleType(), True),\n",
    "    types.StructField('airport_fee', types.DoubleType(), True),\n",
    "    types.StructField('tips', types.DoubleType(), True),\n",
    "    types.StructField('driver_pay', types.DoubleType(), True),\n",
    "    types.StructField('shared_request_flag', types.StringType(), True),\n",
    "    types.StructField('shared_match_flag', types.StringType(), True),\n",
    "    types.StructField('access_a_ride_flag', types.StringType(), True),\n",
    "    types.StructField('wav_request_flag', types.StringType(), True),\n",
    "    types.StructField('wav_match_flag', types.StringType(), True)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_schema = spark.read \\\n",
    "    .option('header', 'true') \\\n",
    "    .schema(schema=schema) \\\n",
    "    .parquet(str(fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', originating_base_num='B02682', request_datetime=datetime.datetime(2021, 1, 1, 8, 28, 9), on_scene_datetime=datetime.datetime(2021, 1, 1, 8, 31, 42), pickup_datetime=datetime.datetime(2021, 1, 1, 8, 33, 44), dropoff_datetime=datetime.datetime(2021, 1, 1, 8, 49, 7), PULocationID=230, DOLocationID=166, trip_miles=5.26, trip_time=923, base_passenger_fare=22.28, tolls=0.0, bcf=0.67, sales_tax=1.98, congestion_surcharge=2.75, airport_fee=None, tips=0.0, driver_pay=14.99, shared_request_flag='N', shared_match_flag='N', access_a_ride_flag=' ', wav_request_flag='N', wav_match_flag='N'),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', originating_base_num='B02682', request_datetime=datetime.datetime(2021, 1, 1, 8, 45, 56), on_scene_datetime=datetime.datetime(2021, 1, 1, 8, 55, 19), pickup_datetime=datetime.datetime(2021, 1, 1, 8, 55, 19), dropoff_datetime=datetime.datetime(2021, 1, 1, 9, 18, 21), PULocationID=152, DOLocationID=167, trip_miles=3.65, trip_time=1382, base_passenger_fare=18.36, tolls=0.0, bcf=0.55, sales_tax=1.63, congestion_surcharge=0.0, airport_fee=None, tips=0.0, driver_pay=17.06, shared_request_flag='N', shared_match_flag='N', access_a_ride_flag=' ', wav_request_flag='N', wav_match_flag='N'),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', originating_base_num='B02764', request_datetime=datetime.datetime(2021, 1, 1, 8, 21, 15), on_scene_datetime=datetime.datetime(2021, 1, 1, 8, 22, 41), pickup_datetime=datetime.datetime(2021, 1, 1, 8, 23, 56), dropoff_datetime=datetime.datetime(2021, 1, 1, 8, 38, 5), PULocationID=233, DOLocationID=142, trip_miles=3.51, trip_time=849, base_passenger_fare=14.05, tolls=0.0, bcf=0.48, sales_tax=1.25, congestion_surcharge=2.75, airport_fee=None, tips=0.94, driver_pay=12.98, shared_request_flag='N', shared_match_flag='N', access_a_ride_flag=' ', wav_request_flag='N', wav_match_flag='N')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_schema.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Couldn't convert from `LongType` to `IntegerType`; parquet complained about expecting `INT64`. Stay as `Long`.\n",
    "\n",
    "## Repartition\n",
    "\n",
    "The whole idea of spark is distributing the dataset and parallelizing the task between the executors in the cluster. Thus we should break out our parquet into smaller *partitions*  so that the workload can be more easily distributed.\n",
    "\n",
    "Otherwise, one executor in the cluster will receive the one dataset, work on it by itself, while the other executors sit idle.\n",
    "\n",
    "Use `df.repartition(int)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lazy eval; will not execute yet\n",
    "# returns a new df that is hash partitioned\n",
    "df_parts = df_with_schema.repartition(24)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not create the directory for `df.write.parquet()`; it will raise `Path already exists` error. Let it create it on its own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = Path(\"../data/taxi_ingest_data/fhvhv_tripdata_2021-01.parquet\")\n",
    "partition_dir = fpath.parent / 'fhvhv' / 'parts'\n",
    "# if not partition_dir.exists():\n",
    "#     partition_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write our df into 24 partitions\n",
    "df_parts.write.parquet(str(partition_dir))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "\n",
    "```bash\n",
    "kohada@Kohada:~/de-zoomcamp/data/taxi_ingest_data$ ls fhvhv/parts/ -lh\n",
    "total 526M\n",
    "-rw-r--r-- 1 kohada kohada   0 Feb 18 00:40 _SUCCESS\n",
    "-rw-r--r-- 1 kohada kohada 22M Feb 18 00:39 part-00000-0c7da773-7f90-44bc-9962-54679a206388-c000.snappy.parquet\n",
    "-rw-r--r-- 1 kohada kohada 22M Feb 18 00:39 part-00001-0c7da773-7f90-44bc-9962-54679a206388-c000.snappy.parquet\n",
    "-rw-r--r-- 1 kohada kohada 22M Feb 18 00:39 part-00002-0c7da773-7f90-44bc-9962-54679a206388-c000.snappy.parquet\n",
    "-rw-r--r-- 1 kohada kohada 22M Feb 18 00:39 part-00003-0c7da773-7f90-44bc-9962-54679a206388-c000.snappy.parquet\n",
    "...\n",
    "-rw-r--r-- 1 kohada kohada 22M Feb 18 00:40 part-00023-0c7da773-7f90-44bc-9962-54679a206388-c000.snappy.parquet\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitions can be read by pointing to the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/18 09:57:07 WARN Utils: Your hostname, Kohada resolves to a loopback address: 127.0.1.1; using 172.30.114.23 instead (on interface eth0)\n",
      "23/02/18 09:57:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/kohada/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/02/18 09:57:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fpath = Path(\"../data/taxi_ingest_data/fhvhv_tripdata_2021-01.parquet\")\n",
    "partition_dir = fpath.parent / 'fhvhv' / 'parts'\n",
    "df = spark.read.parquet(str(partition_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- originating_base_num: string (nullable = true)\n",
      " |-- request_datetime: timestamp (nullable = true)\n",
      " |-- on_scene_datetime: timestamp (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- trip_time: long (nullable = true)\n",
      " |-- base_passenger_fare: double (nullable = true)\n",
      " |-- tolls: double (nullable = true)\n",
      " |-- bcf: double (nullable = true)\n",
      " |-- sales_tax: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- driver_pay: double (nullable = true)\n",
      " |-- shared_request_flag: string (nullable = true)\n",
      " |-- shared_match_flag: string (nullable = true)\n",
      " |-- access_a_ride_flag: string (nullable = true)\n",
      " |-- wav_request_flag: string (nullable = true)\n",
      " |-- wav_match_flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select and filter - lazy\n",
    "df_filter = df.select('pickup_datetime', \n",
    "            'dropoff_datetime', \n",
    "            'PULocationID', \n",
    "            'DOLocationID',) \\\n",
    "            .filter(df.hvfhs_license_num == 'HV0003')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action vs Transformation\n",
    "\n",
    "#### Transformations\n",
    "\n",
    "Lazy eval; not executed until triggered by action\n",
    "\n",
    "- select\n",
    "- filter\n",
    "- join\n",
    "- group by\n",
    "\n",
    "#### Actions\n",
    "\n",
    "Executes the transformations (eager, as opposed to lazy)\n",
    "\n",
    "- show\n",
    "- take (similar to `.head()`)\n",
    "- write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------+------------+\n",
      "|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "|2021-01-12 02:40:22|2021-01-12 03:15:49|         262|         231|\n",
      "|2021-01-05 23:13:22|2021-01-05 23:27:50|          61|         181|\n",
      "|2021-02-01 02:42:09|2021-02-01 02:59:52|         232|           4|\n",
      "|2021-01-28 06:24:36|2021-01-28 06:26:43|          68|          68|\n",
      "|2021-01-30 16:35:46|2021-01-30 16:39:42|         256|         255|\n",
      "|2021-01-16 10:25:35|2021-01-16 10:34:21|          89|          91|\n",
      "|2021-01-11 19:58:23|2021-01-11 20:14:19|          97|          61|\n",
      "|2021-01-03 15:44:58|2021-01-03 16:04:45|          26|         178|\n",
      "|2021-01-15 02:52:00|2021-01-15 03:19:00|         181|         198|\n",
      "|2021-01-09 04:35:35|2021-01-09 05:06:33|          76|          91|\n",
      "|2021-01-15 21:49:48|2021-01-15 22:35:23|         246|          16|\n",
      "|2021-01-27 18:37:56|2021-01-27 18:53:35|         135|          73|\n",
      "|2021-01-12 01:29:44|2021-01-12 01:42:49|          68|         211|\n",
      "|2021-01-25 05:32:15|2021-01-25 05:52:42|         249|         236|\n",
      "|2021-01-27 02:03:39|2021-01-27 02:10:53|          79|           4|\n",
      "|2021-01-07 16:05:32|2021-01-07 16:30:47|          22|          25|\n",
      "|2021-01-13 01:16:00|2021-01-13 01:25:56|          69|         119|\n",
      "|2021-01-17 05:00:39|2021-01-17 05:18:15|         239|         239|\n",
      "|2021-01-15 07:35:14|2021-01-15 07:45:30|          61|          62|\n",
      "|2021-01-18 07:23:11|2021-01-18 07:34:42|         241|          20|\n",
      "+-------------------+-------------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# action\n",
    "df_filter.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark vs SQL\n",
    "\n",
    "The strength of Spark is in the flexibility given by whichever code we're writing our spark worload in. The above snippet is easily achievable in SQL, but with UDF, there's a lot more versatility in what we do in our transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+\n",
      "|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+-----------+------------+------------+------------+\n",
      "| 2021-01-12|  2021-01-12|         262|         231|\n",
      "| 2021-01-05|  2021-01-05|          61|         181|\n",
      "| 2021-01-02|  2021-01-02|         100|           1|\n",
      "| 2021-02-01|  2021-02-01|         232|           4|\n",
      "| 2021-01-06|  2021-01-06|         162|           1|\n",
      "| 2021-01-28|  2021-01-28|          68|          68|\n",
      "| 2021-01-18|  2021-01-18|         205|         205|\n",
      "| 2021-01-30|  2021-01-30|         256|         255|\n",
      "| 2021-01-16|  2021-01-16|          89|          91|\n",
      "| 2021-01-05|  2021-01-05|         132|         102|\n",
      "| 2021-01-11|  2021-01-11|          97|          61|\n",
      "| 2021-01-22|  2021-01-22|          79|          37|\n",
      "| 2021-01-03|  2021-01-03|          26|         178|\n",
      "| 2021-01-15|  2021-01-15|         181|         198|\n",
      "| 2021-01-09|  2021-01-09|          76|          91|\n",
      "| 2021-01-15|  2021-01-15|         246|          16|\n",
      "| 2021-01-27|  2021-01-27|         135|          73|\n",
      "| 2021-01-18|  2021-01-18|          74|         234|\n",
      "| 2021-01-12|  2021-01-12|          68|         211|\n",
      "| 2021-01-25|  2021-01-25|         249|         236|\n",
      "+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adds a column by converting a datetime col to just date\n",
    "df.withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .select('pickup_date',\n",
    "            'dropoff_date', \n",
    "            'PULocationID', \n",
    "            'DOLocationID',) \\\n",
    "    .show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot of pre-defined features in `pyspark.sql.functions` already, but the real strength is in UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user-defined func\n",
    "def not_for_sql(base_num: str) -> str:\n",
    "    \"\"\"Takes basenum and prefix with a letter\n",
    "    based on the numeric portion's divisibility\n",
    "    and appends the hex value to return as string\n",
    "    \"\"\"\n",
    "    # base_num format: [A-Z][some_numbers]\n",
    "    num = int(base_num[1:])\n",
    "    if num % 7 == 0:\n",
    "        return f's/{num:03x}'\n",
    "    elif num % 3 == 0:\n",
    "        return f'a/{num:03x}'\n",
    "    else:\n",
    "        return f'b/{num:03x}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b/f46'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_for_sql('B3910')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_sql_udf = F.udf(not_for_sql, returnType=types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------------+------------+------------+\n",
      "|crazy_base|pickup_date|dropoff_date|PULocationID|DOLocationID|\n",
      "+----------+-----------+------------+------------+------------+\n",
      "|     b/acc| 2021-01-12|  2021-01-12|         262|         231|\n",
      "|     b/a39| 2021-01-05|  2021-01-05|          61|         181|\n",
      "|     b/9ce| 2021-01-02|  2021-01-02|         100|           1|\n",
      "|     b/b42| 2021-02-01|  2021-02-01|         232|           4|\n",
      "|     s/af0| 2021-01-06|  2021-01-06|         162|           1|\n",
      "|     a/b43| 2021-01-28|  2021-01-28|          68|          68|\n",
      "|     b/9ce| 2021-01-18|  2021-01-18|         205|         205|\n",
      "|     b/b35| 2021-01-30|  2021-01-30|         256|         255|\n",
      "|     b/b3b| 2021-01-16|  2021-01-16|          89|          91|\n",
      "|     b/9ce| 2021-01-05|  2021-01-05|         132|         102|\n",
      "|     b/acc| 2021-01-11|  2021-01-11|          97|          61|\n",
      "|     b/9ce| 2021-01-22|  2021-01-22|          79|          37|\n",
      "|     b/b32| 2021-01-03|  2021-01-03|          26|         178|\n",
      "|     a/b49| 2021-01-15|  2021-01-15|         181|         198|\n",
      "|     b/acc| 2021-01-09|  2021-01-09|          76|          91|\n",
      "|     b/b3c| 2021-01-15|  2021-01-15|         246|          16|\n",
      "|     s/b36| 2021-01-27|  2021-01-27|         135|          73|\n",
      "|     b/9ce| 2021-01-18|  2021-01-18|          74|         234|\n",
      "|     b/a39| 2021-01-12|  2021-01-12|          68|         211|\n",
      "|     b/b3f| 2021-01-25|  2021-01-25|         249|         236|\n",
      "+----------+-----------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .withColumn('dropoff_date', F.to_date(df.dropoff_datetime)) \\\n",
    "    .withColumn('crazy_base', not_sql_udf(df.dispatching_base_num)) \\\n",
    "    .select('crazy_base',\n",
    "            'pickup_date',\n",
    "            'dropoff_date', \n",
    "            'PULocationID', \n",
    "            'DOLocationID',) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
