{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resilient Distributed Datasets in Spark\n",
    "\n",
    "It turns out DataFrames are implemented on top of RDDs.\n",
    "\n",
    "- dataframes have schema\n",
    "- RDD is simply a collection of objects\n",
    "\n",
    "## Map and reduce\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/24 14:14:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet('../data/taxi_ingest_data/parts/green/*/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.rdd` returns the underlying RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 23, 12, 33, 7), lpep_dropoff_datetime=datetime.datetime(2020, 1, 23, 12, 38, 42), store_and_fwd_flag='N', RatecodeID=1, PULocationID=24, DOLocationID=41, passenger_count=1, trip_distance=0.9300000071525574, fare_amount=6.0, extra=0.0, mta_tax=0.5, tip_amount=1.3600000143051147, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.30000001192092896, total_amount=8.15999984741211, payment_type=1, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 23, 0, 7, 7), lpep_dropoff_datetime=datetime.datetime(2020, 1, 23, 0, 9, 47), store_and_fwd_flag='N', RatecodeID=1, PULocationID=193, DOLocationID=193, passenger_count=1, trip_distance=0.4699999988079071, fare_amount=4.0, extra=0.5, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.30000001192092896, total_amount=5.300000190734863, payment_type=2, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 29, 12, 7), lpep_dropoff_datetime=datetime.datetime(2020, 1, 29, 12, 28), store_and_fwd_flag=None, RatecodeID=None, PULocationID=33, DOLocationID=49, passenger_count=None, trip_distance=2.1500000953674316, fare_amount=20.93000030517578, extra=2.75, mta_tax=0.0, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.30000001192092896, total_amount=23.979999542236328, payment_type=None, trip_type=None, congestion_surcharge=None)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.rdd.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Row` object is also returned if we `.take` from the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 23, 12, 33, 7), lpep_dropoff_datetime=datetime.datetime(2020, 1, 23, 12, 38, 42), store_and_fwd_flag='N', RatecodeID=1, PULocationID=24, DOLocationID=41, passenger_count=1, trip_distance=0.9300000071525574, fare_amount=6.0, extra=0.0, mta_tax=0.5, tip_amount=1.3600000143051147, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.30000001192092896, total_amount=8.15999984741211, payment_type=1, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 23, 0, 7, 7), lpep_dropoff_datetime=datetime.datetime(2020, 1, 23, 0, 9, 47), store_and_fwd_flag='N', RatecodeID=1, PULocationID=193, DOLocationID=193, passenger_count=1, trip_distance=0.4699999988079071, fare_amount=4.0, extra=0.5, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.30000001192092896, total_amount=5.300000190734863, payment_type=2, trip_type=1, congestion_surcharge=0.0),\n",
       " Row(VendorID=2, lpep_pickup_datetime=datetime.datetime(2020, 1, 29, 12, 7), lpep_dropoff_datetime=datetime.datetime(2020, 1, 29, 12, 28), store_and_fwd_flag=None, RatecodeID=None, PULocationID=33, DOLocationID=49, passenger_count=None, trip_distance=2.1500000953674316, fare_amount=20.93000030517578, extra=2.75, mta_tax=0.0, tip_amount=0.0, tolls_amount=0.0, ehail_fee=None, improvement_surcharge=0.30000001192092896, total_amount=23.979999542236328, payment_type=None, trip_type=None, congestion_surcharge=None)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['lpep_pickup_datetime', 'PULocationID', 'total_amount']\n",
    "rdd_green = df_green \\\n",
    "                .select(*cols) \\\n",
    "                .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(year=2020, month=1, day=1)\n",
    "def filter_datetime(row):\n",
    "    \"\"\"\n",
    "    pyspark.RDD.filter callable\n",
    "    only accepts row as argument\n",
    "    \"\"\"\n",
    "    return row.lpep_pickup_datetime >= start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_groupby(row):\n",
    "    \"\"\"\n",
    "    Map of MapReduce\n",
    "    pyspark.RDD.map callable\n",
    "    only accepts row as arg\n",
    "    \"\"\"\n",
    "    # assigning our keys\n",
    "    # truncate to datetime to hour\n",
    "    hour = row.lpep_pickup_datetime.replace(minute=0, second=0, microsecond=0)\n",
    "    zone = row.PULocationID\n",
    "    # key becomes a tuple\n",
    "    key = (hour, zone)\n",
    "\n",
    "    # assigning values \n",
    "    amount = row.total_amount\n",
    "    count = 1\n",
    "    # value's also a tuple so that we have a 1:1 key: value relation\n",
    "    value = (amount, count)\n",
    "    return (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_revenue(left_val, right_val):\n",
    "    \"\"\"\n",
    "    Reduce of MapReduce\n",
    "    pyspark.RDD.reduceByKey callable\n",
    "    The left, right terminalogy is identical to itertools.reduce\n",
    "    where the reduce() func iterates through an array from left to right,\n",
    "    collecting the results starting from left\n",
    "\n",
    "    Since this will be used in reduceByKey, our call do not need\n",
    "    to specify which keys to use; all unique keys will be used\n",
    "    \"\"\"\n",
    "    # unpack our value\n",
    "    # left is the cumulative val, so far\n",
    "    left_amt, left_cnt = left_val\n",
    "    # right is the new val\n",
    "    right_amt, right_cnt = right_val\n",
    "\n",
    "    # add them\n",
    "    output_amt = left_amt + right_amt\n",
    "    output_cnt = left_cnt + right_cnt\n",
    "\n",
    "    # return as tuple to match existing format\n",
    "    return (output_amt, output_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((datetime.datetime(2020, 1, 3, 16, 0), 75), (1449.6299991607666, 87)),\n",
       " ((datetime.datetime(2020, 1, 11, 11, 0), 193), (43.55000042915344, 6)),\n",
       " ((datetime.datetime(2020, 1, 2, 14, 0), 263), (76.92000198364258, 4)),\n",
       " ((datetime.datetime(2020, 1, 4, 17, 0), 129), (524.7999963760376, 28)),\n",
       " ((datetime.datetime(2020, 1, 9, 10, 0), 123), (60.64000129699707, 2)),\n",
       " ((datetime.datetime(2020, 1, 1, 10, 0), 127), (23.850000381469727, 2)),\n",
       " ((datetime.datetime(2020, 1, 10, 18, 0), 181), (357.79000091552734, 17)),\n",
       " ((datetime.datetime(2020, 1, 22, 20, 0), 74), (655.1300024986267, 48)),\n",
       " ((datetime.datetime(2020, 1, 26, 12, 0), 76), (106.87999725341797, 6)),\n",
       " ((datetime.datetime(2020, 1, 3, 19, 0), 97), (769.689998626709, 46))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_green \\\n",
    "    .filter(filter_datetime) \\\n",
    "    .map(prepare_for_groupby) \\\n",
    "    .reduceByKey(calc_revenue) \\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our `revenue_amount` and `trip_counts` grouped by `hour` and `pickup_zone`. We can add a cosmetic fix to ungroup the `key-val` tuples to flatten it, and return it to dataframe by adding the schema as a `namedtuple` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "RevenueRow = namedtuple('RevenueRow', ['hour', 'pickup_zone', 'revenue', 'count'])\n",
    "def unwrap(row):\n",
    "    return RevenueRow(\n",
    "        hour=row[0][0], \n",
    "        pickup_zone=row[0][1], \n",
    "        revenue=row[1][0], \n",
    "        count=row[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------------+-----+\n",
      "|               hour|pickup_zone|           revenue|count|\n",
      "+-------------------+-----------+------------------+-----+\n",
      "|2020-01-03 16:00:00|         75|1449.6299991607666|   87|\n",
      "|2020-01-11 11:00:00|        193| 43.55000042915344|    6|\n",
      "|2020-01-02 14:00:00|        263| 76.92000198364258|    4|\n",
      "|2020-01-04 17:00:00|        129| 524.7999963760376|   28|\n",
      "|2020-01-09 10:00:00|        123| 60.64000129699707|    2|\n",
      "|2020-01-01 10:00:00|        127|23.850000381469727|    2|\n",
      "|2020-01-10 18:00:00|        181|357.79000091552734|   17|\n",
      "|2020-01-22 20:00:00|         74| 655.1300024986267|   48|\n",
      "|2020-01-26 12:00:00|         76|106.87999725341797|    6|\n",
      "|2020-01-03 19:00:00|         97|  769.689998626709|   46|\n",
      "|2020-01-27 16:00:00|         65| 622.7200040817261|   31|\n",
      "|2020-01-04 00:00:00|        166|145.34999990463257|   10|\n",
      "|2020-01-09 07:00:00|         89|295.28000354766846|   11|\n",
      "|2020-01-02 10:00:00|        213|119.89000129699707|    6|\n",
      "|2020-01-23 16:00:00|        166| 974.9999933242798|   58|\n",
      "|2020-01-26 22:00:00|         25| 190.9900016784668|   11|\n",
      "|2020-01-07 12:00:00|        258|51.849998474121094|    2|\n",
      "|2020-01-14 10:00:00|         52|197.18999767303467|   17|\n",
      "|2020-01-29 09:00:00|         72|166.17000007629395|    8|\n",
      "|2020-01-09 07:00:00|        181|114.46000099182129|    6|\n",
      "+-------------------+-----------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result = rdd_green \\\n",
    "    .filter(filter_datetime) \\\n",
    "    .map(prepare_for_groupby) \\\n",
    "    .reduceByKey(calc_revenue) \\\n",
    "    .map(unwrap) \\\n",
    "    .toDF()\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hour', TimestampType(), True), StructField('pickup_zone', LongType(), True), StructField('revenue', DoubleType(), True), StructField('count', LongType(), True)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recasting the schema during `toDF()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "res_schema = types.StructType([\n",
    "    types.StructField('hour', types.TimestampType(), True), \n",
    "    types.StructField('pickup_zone', types.IntegerType(), True), \n",
    "    types.StructField('revenue', types.FloatType(), True), \n",
    "    types.StructField('count', types.IntegerType(), True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+---------+-----+\n",
      "|               hour|pickup_zone|  revenue|count|\n",
      "+-------------------+-----------+---------+-----+\n",
      "|2020-01-03 16:00:00|         75|  1449.63|   87|\n",
      "|2020-01-11 11:00:00|        193|    43.55|    6|\n",
      "|2020-01-02 14:00:00|        263|    76.92|    4|\n",
      "|2020-01-04 17:00:00|        129|    524.8|   28|\n",
      "|2020-01-09 10:00:00|        123|    60.64|    2|\n",
      "|2020-01-01 10:00:00|        127|    23.85|    2|\n",
      "|2020-01-10 18:00:00|        181|   357.79|   17|\n",
      "|2020-01-22 20:00:00|         74|   655.13|   48|\n",
      "|2020-01-26 12:00:00|         76|   106.88|    6|\n",
      "|2020-01-03 19:00:00|         97|   769.69|   46|\n",
      "|2020-01-27 16:00:00|         65|622.72003|   31|\n",
      "|2020-01-04 00:00:00|        166|   145.35|   10|\n",
      "|2020-01-09 07:00:00|         89|   295.28|   11|\n",
      "|2020-01-02 10:00:00|        213|   119.89|    6|\n",
      "|2020-01-23 16:00:00|        166|    975.0|   58|\n",
      "|2020-01-26 22:00:00|         25|   190.99|   11|\n",
      "|2020-01-07 12:00:00|        258|    51.85|    2|\n",
      "|2020-01-14 10:00:00|         52|   197.19|   17|\n",
      "|2020-01-29 09:00:00|         72|   166.17|    8|\n",
      "|2020-01-09 07:00:00|        181|   114.46|    6|\n",
      "+-------------------+-----------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# all the operations\n",
    "df_result = rdd_green \\\n",
    "    .filter(filter_datetime) \\\n",
    "    .map(prepare_for_groupby) \\\n",
    "    .reduceByKey(calc_revenue) \\\n",
    "    .map(unwrap) \\\n",
    "    .toDF(schema=res_schema)\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job detail will show two stages:\n",
    "\n",
    "1. Map\n",
    "2. Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('hour', TimestampType(), True), StructField('pickup_zone', IntegerType(), True), StructField('revenue', FloatType(), True), StructField('count', IntegerType(), True)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.write.parquet('../data/reports/revenue/green_zones/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD mapPartitions\n",
    "\n",
    "Signature: mapPartitions(`func`)\n",
    "\n",
    "Description:\n",
    "\n",
    "> Similar to map, but runs separately on each partition (block) of the RDD, so func must be of type `Iterator<T> => Iterator<U>` when running on an RDD of type T.\n",
    "\n",
    "\n",
    "`map` takes a func, and applies it to consume one element in the dataset to produce another element\n",
    "\n",
    "`mapPartitions` takes a *partition* to return *another partition*\n",
    "\n",
    "> RDD partition -> `mapPartitions` -> RDD partition\n",
    "\n",
    "So why use it over `map`? When the dataset cannot fit in memory. If we had 1TB to `map` a prediction model, there wouldn't be enough RAM. `mapPartitions` can divvy up the dataset to apply it partition by partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['VendorID', 'lpep_pickup_datetime', 'PULocationID', 'DOLocationID', 'trip_distance']\n",
    "\n",
    "# get RDD to make duration predictions on\n",
    "duration_rdd = df_green \\\n",
    "    .select(columns) \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "def apply_count_in_batch(part):\n",
    "    \"\"\"\n",
    "    Applying to a partition\n",
    "    Must return an iterator\n",
    "    \"\"\"\n",
    "    # left is being accumulated as reduce iterates through part\n",
    "    return [reduce(lambda left, _: left + 1, part, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1314009, 573315, 525878, 389729]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_rdd.mapPartitions(apply_count_in_batch).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unevenness of the partitions will affect the execution times; the first partition is 3x larger and will take 3x longer. Could mitigate by repartitioning, but that is also an expensive operation...\n",
    "\n",
    "instead of passing a spark partition, we could also pass a dataframe so that pandas operations can be leveraged for our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>trip_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-23 12:33:07</td>\n",
       "      <td>24</td>\n",
       "      <td>41</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-23 00:07:07</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-29 12:07:00</td>\n",
       "      <td>33</td>\n",
       "      <td>49</td>\n",
       "      <td>2.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-03 15:22:00</td>\n",
       "      <td>224</td>\n",
       "      <td>45</td>\n",
       "      <td>2.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-10 06:49:08</td>\n",
       "      <td>74</td>\n",
       "      <td>75</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-27 18:02:49</td>\n",
       "      <td>226</td>\n",
       "      <td>129</td>\n",
       "      <td>3.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-27 07:53:12</td>\n",
       "      <td>74</td>\n",
       "      <td>48</td>\n",
       "      <td>6.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-03 16:03:57</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-04 19:17:09</td>\n",
       "      <td>129</td>\n",
       "      <td>68</td>\n",
       "      <td>7.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-06 15:55:47</td>\n",
       "      <td>61</td>\n",
       "      <td>225</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID lpep_pickup_datetime  PULocationID  DOLocationID  trip_distance\n",
       "0         2  2020-01-23 12:33:07            24            41           0.93\n",
       "1         2  2020-01-23 00:07:07           193           193           0.47\n",
       "2         2  2020-01-29 12:07:00            33            49           2.15\n",
       "3         2  2020-01-03 15:22:00           224            45           2.35\n",
       "4         2  2020-01-10 06:49:08            74            75           1.43\n",
       "5         2  2020-01-27 18:02:49           226           129           3.65\n",
       "6         2  2020-01-27 07:53:12            74            48           6.07\n",
       "7         2  2020-01-03 16:03:57            75            75           0.82\n",
       "8         2  2020-01-04 19:17:09           129            68           7.38\n",
       "9         2  2020-01-06 15:55:47            61           225           0.48"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_rows = duration_rdd.take(10) # returns list[Rows]\n",
    "# if columns were not specified, pd does not have the schema\n",
    "df = pd.DataFrame(some_rows, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(df):\n",
    "    \"\"\"\n",
    "    Some model that returns a series of predictions given a dataframe\n",
    "    of features\n",
    "    \"\"\"\n",
    "    # y_pred = model.predict(df)\n",
    "    y_pred = df.trip_distance * 3\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_in_batch(part):\n",
    "    \"\"\"\n",
    "    Applying to a partition\n",
    "    Must return an iterator\n",
    "    \"\"\"\n",
    "    # columns is some var defined outside func scope\n",
    "    df = pd.DataFrame(part, columns=columns)\n",
    "\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['lpep_pickup_datetime'])\n",
    "    df = df.drop(columns=['lpep_pickup_datetime'])\n",
    "    preds = model_predict(df)\n",
    "    \n",
    "    # add col to df\n",
    "    df['pred_duration'] = preds\n",
    "    # returns iterator where each row becomes a namedtuple\n",
    "    # namedtuples are iterables and attributes can be accessed by name\n",
    "    for row in df.itertuples(index=False,name=\"duration_features\"):\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-01-01'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = namedtuple('features', ['datetime'])\n",
    "bar = foo(datetime='2020-01-01')\n",
    "bar.datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bar\u001b[39m.\u001b[39;49mdatetime \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(bar\u001b[39m.\u001b[39mdatetime)\n\u001b[1;32m      2\u001b[0m bar\u001b[39m.\u001b[39mdatetime\n",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "source": [
    "bar.datetime = pd.to_datetime(bar.datetime)\n",
    "bar.datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through `itertuples` and accessing fields by attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-01-23 12:33:07')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_dts = [row.lpep_pickup_datetime \n",
    "           for row in df.itertuples(index=False, name=\"duration_features\")]\n",
    "pd.to_datetime(rdd_dts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds_schema = types.StructType([\n",
    "    types.StructField('VendorID', types.IntegerType(), True), \n",
    "    types.StructField('PULocationID', types.IntegerType(), True), \n",
    "    types.StructField('DOLocationID', types.IntegerType(), True), \n",
    "    types.StructField('trip_distance', types.FloatType(), True), \n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True), \n",
    "    types.StructField('pred_duration', types.FloatType(), True)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 1, 23, 12, 33, 7)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_rdd.take(1)[0].lpep_pickup_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_preds = duration_rdd \\\n",
    "    .mapPartitions(apply_model_in_batch) \\\n",
    "    .toDF() # can set schema here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------+-------------------+---------------+------------------+\n",
      "|VendorID|PULocationID|DOLocationID|      trip_distance|pickup_datetime|     pred_duration|\n",
      "+--------+------------+------------+-------------------+---------------+------------------+\n",
      "|       2|          24|          41| 0.9300000071525574|             {}| 2.790000021457672|\n",
      "|       2|         193|         193| 0.4699999988079071|             {}|1.4099999964237213|\n",
      "|       2|          33|          49| 2.1500000953674316|             {}| 6.450000286102295|\n",
      "|       2|         224|          45| 2.3499999046325684|             {}| 7.049999713897705|\n",
      "|       2|          74|          75| 1.4299999475479126|             {}| 4.289999842643738|\n",
      "|       2|         226|         129| 3.6500000953674316|             {}|10.950000286102295|\n",
      "|       2|          74|          48|  6.070000171661377|             {}| 18.21000051498413|\n",
      "|       2|          75|          75| 0.8199999928474426|             {}| 2.459999978542328|\n",
      "|       2|         129|          68|  7.380000114440918|             {}|22.140000343322754|\n",
      "|       2|          61|         225|0.47999998927116394|             {}|1.4399999678134918|\n",
      "|       2|           7|         223| 1.3600000143051147|             {}| 4.080000042915344|\n",
      "|       2|          81|          95| 14.399999618530273|             {}| 43.19999885559082|\n",
      "|       2|         193|         141|  2.069999933242798|             {}|6.2099997997283936|\n",
      "|       2|          33|          65| 0.5199999809265137|             {}| 1.559999942779541|\n",
      "|       1|         116|         244| 1.2000000476837158|             {}|3.6000001430511475|\n",
      "|       2|         223|         138|  3.200000047683716|             {}| 9.600000143051147|\n",
      "|       2|         244|          56| 14.699999809265137|             {}| 44.09999942779541|\n",
      "|       2|          97|         231| 2.6500000953674316|             {}| 7.950000286102295|\n",
      "|       2|          41|         166| 1.1799999475479126|             {}| 3.539999842643738|\n",
      "|       2|          82|         198|  4.130000114440918|             {}|12.390000343322754|\n",
      "+--------+------------+------------+-------------------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|     pred_duration|\n",
      "+------------------+\n",
      "| 2.790000021457672|\n",
      "|1.4099999964237213|\n",
      "| 6.450000286102295|\n",
      "| 7.049999713897705|\n",
      "| 4.289999842643738|\n",
      "|10.950000286102295|\n",
      "| 18.21000051498413|\n",
      "| 2.459999978542328|\n",
      "|22.140000343322754|\n",
      "|1.4399999678134918|\n",
      "| 4.080000042915344|\n",
      "| 43.19999885559082|\n",
      "|6.2099997997283936|\n",
      "| 1.559999942779541|\n",
      "|3.6000001430511475|\n",
      "| 9.600000143051147|\n",
      "| 44.09999942779541|\n",
      "| 7.950000286102295|\n",
      "| 3.539999842643738|\n",
      "|12.390000343322754|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_preds.select('pred_duration').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('VendorID', LongType(), True), StructField('lpep_pickup_datetime', StructType([]), True), StructField('PULocationID', LongType(), True), StructField('DOLocationID', LongType(), True), StructField('trip_distance', DoubleType(), True), StructField('pred_duration', DoubleType(), True)])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preds.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part = \"abcde\"\n",
    "apply_model_in_batch(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "a5353673437182399eff36f4c16ed56ea0c7acdfe0ac221ac5d31504a99f322a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
